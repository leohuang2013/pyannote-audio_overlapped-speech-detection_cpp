# speaker_diarization_prototype

Convert pyannote-audio's speaker diarization pipeline to C++.

Whole pipeline is splitted into 3 stages,
- segment
- embedding
- clustering

For segment and embedding part,
1. export embedding and segmentation model,
2. convert pre-embedding, post-embedding, pre-segment, post-segment.

For clustering, convert all python code to C++ code.

# Model 

## Export model
- segmenation
segment/export2.py

- embedding
embeddings/export3.py

Create conda evironment
```
$> conda create --name sd_embeddings
$> conda activate sd_embeddings
$> cd segment
$> python export2.py
$> cd embedding
$> python export3.py
```

# Dependencies
- onnxruntime
- liborch
onnxruntime is used to inference for Segmentation and Embeddings. 
liborch is used to calculate STFT. STFT is part of segmentation module, we can export segmentation as whole, including STFT, however, 
there gonna be to relative big difference. It cannot even pass the verification which compare segmentation result generated by using
original model with pytorch and exported onnx model with onnxruntime in python. This is part is done in segment/export2.py.

Note: even verification passed in python code - export2.py, there are still tiny difference between result gererated by python onnxruntime 
and onnxruntime C++. For embeddings, absolute difference is 0.02. relative difference is 0.1, since clustering stage is very senstive 
to tiny difffernce which will leads to different result compared with result generated by pyannote-audio.

# Build

```
$> cd pipline
$> mkdir build && cd build
$> cmake ..
$> make
```
If want build GPU version
```
$> cmake -DGPU=ON ..
$> make
```

# Run
./speakerDiarizer ../../segment/segment2.onnx ../../embeddings/emd4.onnx ~/storage/sharedFolderVirtualbox/audioForTesting/english_15s16k.wav

# onnxruntime to GPU
It seems no need change code, instead set cuda when convert to model. For segment.onnx model, change source 
a) add following line to model = ....
model.cuda() 
b) change 
dummy_input = torch.zeros(3, 1, 32000)
-->
dummy_input = torch.zeros(3, 1, 32000).cuda()

change onnx.cmake to download gpu version of onnxruntime

# Verification
Since whole project is to translate pyannote-audio speaker diarization pipleline from python to C++, strategy I adopted here is 
write input/output of each small function in python to txt file, and do same for C++, then load txt file into python to compare 
and check difference. Target is to make each input and output is same.
For this purpose, script/verifyEveryStepResult.py is created.
``` bash
$> python verifyEveryStepResult.py
```
Above command is to compare txt files generated /tmp. and command below is to delete all the txt files.
``` bash
$> python verifyEveryStepResult.py clean
```


# For hierichical clustering
tried following
- hclust-cpp/fastcluster
https://github.com/cdalitz/hclust-cpp
result is wrong, including distance of clusters and result 'fcluster'

- agglomerative-hierarchical-clustering
https://github.com/gyaikhom/agglomerative-hierarchical-clustering/tree/master
centroid_linkage is empty

- alglib
https://www.alglib.net/dataanalysis/clustering.php
does not support centriod

python code used is,
/home/leo/product/speaker_diarization/diarization/lib/python3.10/site-packages/pyannote

There is a lot code added. Search "LIYI" by command grep to find them

- k-means library
https://github.com/aditya1601/kmeans-clustering-cpp


